{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ApacheSpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPO1abhYS+uuxB7FV1D0ijy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phuongtranhuy/APACHE_SPARK/blob/main/ApacheSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqALOZB6qthq",
        "outputId": "9e2d23c5-5ba0-4b4b-c5f8-3c11c9f93757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "85kEjQY1rDrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz -C /content/notebooks"
      ],
      "metadata": {
        "id": "b1qhzpsjMxZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/notebooks/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "# install findspark using pip\n"
      ],
      "metadata": {
        "id": "wjaU_ZFCuqG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4JOQitk9s8wm",
        "outputId": "6e2b86e2-1efc-4094-b687-2981404a9f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/notebooks/spark-3.0.0-bin-hadoop3.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
        "sc = SparkContext.getOrCreate(conf)\n",
        "\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    age = int(fields[2])\n",
        "    numFriends = int(fields[3])\n",
        "    return (age, numFriends)\n",
        "\n",
        "lines = sc.textFile('/content/drive/My Drive/Colab Notebooks/fakefriends.csv')\n",
        "rdd = lines.map(parseLine)\n",
        "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
        "results = averagesByAge.collect()\n",
        "results.sort(key=lambda x:x[0])\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jekSw6DT3OKX",
        "outputId": "ef265489-3b99-4ba1-d7aa-1a2671d3aa1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18, 343.375)\n",
            "(19, 213.27272727272728)\n",
            "(20, 165.0)\n",
            "(21, 350.875)\n",
            "(22, 206.42857142857142)\n",
            "(23, 246.3)\n",
            "(24, 233.8)\n",
            "(25, 197.45454545454547)\n",
            "(26, 242.05882352941177)\n",
            "(27, 228.125)\n",
            "(28, 209.1)\n",
            "(29, 215.91666666666666)\n",
            "(30, 235.8181818181818)\n",
            "(31, 267.25)\n",
            "(32, 207.9090909090909)\n",
            "(33, 325.3333333333333)\n",
            "(34, 245.5)\n",
            "(35, 211.625)\n",
            "(36, 246.6)\n",
            "(37, 249.33333333333334)\n",
            "(38, 193.53333333333333)\n",
            "(39, 169.28571428571428)\n",
            "(40, 250.8235294117647)\n",
            "(41, 268.55555555555554)\n",
            "(42, 303.5)\n",
            "(43, 230.57142857142858)\n",
            "(44, 282.1666666666667)\n",
            "(45, 309.53846153846155)\n",
            "(46, 223.69230769230768)\n",
            "(47, 233.22222222222223)\n",
            "(48, 281.4)\n",
            "(49, 184.66666666666666)\n",
            "(50, 254.6)\n",
            "(51, 302.14285714285717)\n",
            "(52, 340.6363636363636)\n",
            "(53, 222.85714285714286)\n",
            "(54, 278.0769230769231)\n",
            "(55, 295.53846153846155)\n",
            "(56, 306.6666666666667)\n",
            "(57, 258.8333333333333)\n",
            "(58, 116.54545454545455)\n",
            "(59, 220.0)\n",
            "(60, 202.71428571428572)\n",
            "(61, 256.22222222222223)\n",
            "(62, 220.76923076923077)\n",
            "(63, 384.0)\n",
            "(64, 281.3333333333333)\n",
            "(65, 298.2)\n",
            "(66, 276.44444444444446)\n",
            "(67, 214.625)\n",
            "(68, 269.6)\n",
            "(69, 235.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "def mapper(line):\n",
        "    fields = line.split(',')\n",
        "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), \\\n",
        "               age=int(fields[2]), numFriends=int(fields[3]))\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"/content/drive/My Drive/Colab Notebooks/fakefriends.csv\")\n",
        "people = lines.map(mapper)\n",
        "\n",
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people).cache()\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
        "for teen in teenagers.collect():\n",
        "  print(teen)\n",
        "\n",
        "# We can also use functions instead of SQL queries:\n",
        "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
        "\n",
        "spark.stop()\n",
        "#print(people.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OazeqVBl3tUI",
        "outputId": "ca846f26-f84e-48e0-ad6d-90772f245e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(ID=21, name=\"b'Miles'\", age=19, numFriends=268)\n",
            "Row(ID=52, name=\"b'Beverly'\", age=19, numFriends=269)\n",
            "Row(ID=54, name=\"b'Brunt'\", age=19, numFriends=5)\n",
            "Row(ID=106, name=\"b'Beverly'\", age=18, numFriends=499)\n",
            "Row(ID=115, name=\"b'Dukat'\", age=18, numFriends=397)\n",
            "Row(ID=133, name=\"b'Quark'\", age=19, numFriends=265)\n",
            "Row(ID=136, name=\"b'Will'\", age=19, numFriends=335)\n",
            "Row(ID=225, name=\"b'Elim'\", age=19, numFriends=106)\n",
            "Row(ID=304, name=\"b'Will'\", age=19, numFriends=404)\n",
            "Row(ID=341, name=\"b'Data'\", age=18, numFriends=326)\n",
            "Row(ID=366, name=\"b'Keiko'\", age=19, numFriends=119)\n",
            "Row(ID=373, name=\"b'Quark'\", age=19, numFriends=272)\n",
            "Row(ID=377, name=\"b'Beverly'\", age=18, numFriends=418)\n",
            "Row(ID=404, name=\"b'Kasidy'\", age=18, numFriends=24)\n",
            "Row(ID=409, name=\"b'Nog'\", age=19, numFriends=267)\n",
            "Row(ID=439, name=\"b'Data'\", age=18, numFriends=417)\n",
            "Row(ID=444, name=\"b'Keiko'\", age=18, numFriends=472)\n",
            "Row(ID=492, name=\"b'Dukat'\", age=19, numFriends=36)\n",
            "Row(ID=494, name=\"b'Kasidy'\", age=18, numFriends=194)\n",
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 18|    8|\n",
            "| 19|   11|\n",
            "| 20|    5|\n",
            "| 21|    8|\n",
            "| 22|    7|\n",
            "| 23|   10|\n",
            "| 24|    5|\n",
            "| 25|   11|\n",
            "| 26|   17|\n",
            "| 27|    8|\n",
            "| 28|   10|\n",
            "| 29|   12|\n",
            "| 30|   11|\n",
            "| 31|    8|\n",
            "| 32|   11|\n",
            "| 33|   12|\n",
            "| 34|    6|\n",
            "| 35|    8|\n",
            "| 36|   10|\n",
            "| 37|    9|\n",
            "+---+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
        "sc = SparkContext.getOrCreate(conf)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    stationID = fields[0]\n",
        "    entryType = fields[2]\n",
        "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
        "    return (stationID, entryType, temperature)\n",
        "\n",
        "lines = sc.textFile(\"/content/drive/My Drive/Colab Notebooks/1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "minTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
        "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
        "minTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
        "results = minTemps.collect();\n",
        "\n",
        "for result in results:\n",
        "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykq8AiN-8lnw",
        "outputId": "85c1f2bb-7a04-4568-eed3-ce3366f41710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ITE00100554\t90.14F\n",
            "EZE00100082\t90.14F\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyodbc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEf96oCTOEZe",
        "outputId": "ea2a815b-bad4-42ff-82a9-2a980b4a6770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyodbc\n",
            "  Downloading pyodbc-4.0.32.tar.gz (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 28.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyodbc\n",
            "  Building wheel for pyodbc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyodbc: filename=pyodbc-4.0.32-cp37-cp37m-linux_x86_64.whl size=287370 sha256=84ca7ee0f26014721a1e4a058f2f6d8af5e996e9436ea23ebdbfa07230ab101d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/9c/da/8652fd42e0f662015554f00a9e96fe4f438dfd1ef59787879e\n",
            "Successfully built pyodbc\n",
            "Installing collected packages: pyodbc\n",
            "Successfully installed pyodbc-4.0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "pyodbc.__file__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6vfpL8_8ORV1",
        "outputId": "61a5dc17-b5ec-4909-e3fa-e27399ccdb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.7/dist-packages/pyodbc.cpython-37m-x86_64-linux-gnu.so'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Application UI: http://localhost:4040/\n",
        "Resource Manager: http://localhost:9870\n",
        "Spark JobTracker: http://localhost:8088/\n",
        "Node Specific Info: http://localhost:8042/"
      ],
      "metadata": {
        "id": "jp7lz9SN66GY"
      }
    }
  ]
}