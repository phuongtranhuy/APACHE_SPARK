{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ApacheSpark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNz+A2sR7FPpIkgBrfHkhy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phuongtranhuy/APACHE_SPARK/blob/main/ApacheSpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqALOZB6qthq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)\n",
        "\n",
        "lib_path = '/content/notebooks/Py_Lib'\n",
        "sys.path.insert(0,lib_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "#!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "85kEjQY1rDrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz -C /content/notebooks"
      ],
      "metadata": {
        "id": "b1qhzpsjMxZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/notebooks/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "# install findspark using pip\n"
      ],
      "metadata": {
        "id": "wjaU_ZFCuqG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -q findspark\n",
        "lib_path = '/content/notebooks/Py_Lib'\n",
        "sys.path.insert(0,lib_path)\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "4JOQitk9s8wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
        "sc = SparkContext.getOrCreate(conf)\n",
        "\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    age = int(fields[2])\n",
        "    numFriends = int(fields[3])\n",
        "    return (age, numFriends)\n",
        "\n",
        "lines = sc.textFile('/content/drive/My Drive/Colab Notebooks/fakefriends.csv')\n",
        "rdd = lines.map(parseLine)\n",
        "totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
        "results = averagesByAge.collect()\n",
        "results.sort(key=lambda x:x[0])\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "jekSw6DT3OKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
        "\n",
        "def mapper(line):\n",
        "    fields = line.split(',')\n",
        "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), \\\n",
        "               age=int(fields[2]), numFriends=int(fields[3]))\n",
        "\n",
        "lines = spark.sparkContext.textFile(\"/content/drive/My Drive/Colab Notebooks/fakefriends.csv\")\n",
        "people = lines.map(mapper)\n",
        "\n",
        "# Infer the schema, and register the DataFrame as a table.\n",
        "schemaPeople = spark.createDataFrame(people).cache()\n",
        "schemaPeople.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# SQL can be run over DataFrames that have been registered as a table.\n",
        "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
        "\n",
        "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
        "for teen in teenagers.collect():\n",
        "  print(teen)\n",
        "\n",
        "# We can also use functions instead of SQL queries:\n",
        "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
        "\n",
        "spark.stop()\n",
        "#print(people.take(5))"
      ],
      "metadata": {
        "id": "OazeqVBl3tUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
        "sc = SparkContext.getOrCreate(conf)\n",
        "\n",
        "def parseLine(line):\n",
        "    fields = line.split(',')\n",
        "    stationID = fields[0]\n",
        "    entryType = fields[2]\n",
        "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
        "    return (stationID, entryType, temperature)\n",
        "\n",
        "lines = sc.textFile(\"/content/drive/My Drive/Colab Notebooks/1800.csv\")\n",
        "parsedLines = lines.map(parseLine)\n",
        "minTemps = parsedLines.filter(lambda x: \"TMAX\" in x[1])\n",
        "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
        "minTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))\n",
        "results = minTemps.collect();\n",
        "\n",
        "for result in results:\n",
        "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))"
      ],
      "metadata": {
        "id": "ykq8AiN-8lnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyodbc\n",
        "!pip install --target=$lib_path pyodbc"
      ],
      "metadata": {
        "id": "xEf96oCTOEZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyodbc\n",
        "import pandas as pd\n",
        "pyodbc.__file__"
      ],
      "metadata": {
        "id": "6vfpL8_8ORV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "A7lmoKk7tfwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Application UI: http://localhost:4040/\n",
        "Resource Manager: http://localhost:9870\n",
        "Spark JobTracker: http://localhost:8088/\n",
        "Node Specific Info: http://localhost:8042/"
      ],
      "metadata": {
        "id": "jp7lz9SN66GY"
      }
    }
  ]
}